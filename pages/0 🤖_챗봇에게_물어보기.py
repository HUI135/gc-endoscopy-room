import streamlit as st
from langchain_community.document_loaders import GitLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain.vectorstores import FAISS
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from openai import OpenAI
import streamlit as st

# secrets ê²€ì‚¬ (ì§€ê¸ˆ ì“°ì‹  ê²ƒ ìœ ì§€ ê°€ëŠ¥)
if "gpt" not in st.secrets or "openai_api_key" not in st.secrets["gpt"]:
    st.write("í˜„ì¬ secrets í‚¤ë“¤:", list(st.secrets.keys()))
    st.write("gpt ì„¹ì…˜:", st.secrets.get("gpt"))
    raise KeyError('secretsì— [gpt].openai_api_keyê°€ ì—†ìŠµë‹ˆë‹¤.')

OPENAI_API_KEY = st.secrets["gpt"]["openai_api_key"]

# âœ… í•µì‹¬: í´ë¼ì´ì–¸íŠ¸ ë§Œë“¤ ë•Œ api_keyë¥¼ ì§ì ‘ ì „ë‹¬
client = OpenAI(api_key=OPENAI_API_KEY)

# ì˜ˆì‹œ í˜¸ì¶œ
resp = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "hello"}]
)
st.write(resp.choices[0].message.content)

# GitHub ë¦¬í¬ì§€í† ë¦¬ URLê³¼ ë¸Œëœì¹˜ (ë‹¹ì‹ ì˜ ë¦¬í¬ë¡œ ë°”ê¾¸ì„¸ìš”)
REPO_URL = "https://github.com/HUI135/gc-endoscopy-room.git"
BRANCH = "main"

# API í‚¤ (Streamlit ì‹œí¬ë¦¿ìœ¼ë¡œ ê´€ë¦¬ ì¶”ì²œ)
OPENAI_API_KEY = st.secrets["gpt"]["openai_api_key"]

# ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ í•¨ìˆ˜ (ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ)
@st.cache_resource
def load_knowledge_base():
    # Git ë¦¬í¬ì§€í† ë¦¬ í´ë¡  ë° ë¡œë“œ
    loader = GitLoader(
        clone_url=REPO_URL,
        repo_path="./temp_repo",  # ì„ì‹œ í´ë”
        branch=BRANCH,
        file_filter=lambda file_path: file_path.endswith((".py", ".md", ".txt"))  # ì›í•˜ëŠ” íŒŒì¼ë§Œ ë¡œë“œ
    )
    docs = loader.load()

    # í…ìŠ¤íŠ¸ ìª¼ê°œê¸°
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)

    # ì„ë² ë”©ê³¼ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
    embeddings = OpenAIEmbeddings(OPENAI_API_KEY=OPENAI_API_KEY)
    vectorstore = FAISS.from_documents(splits, embeddings)

    return vectorstore

# ë©”ì¸ ì•±
st.title("ğŸ¤– ì±—ë´‡ì—ê²Œ ë¬¼ì–´ë³´ê¸°")
st.subheader("ğŸ¤– ì±—ë´‡ì—ê²Œ ë¬¼ì–´ë³´ê¸°", divider='rainbow')

# ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ
vectorstore = load_knowledge_base()

# LLM ì„¤ì •
llm = ChatOpenAI(model="gpt-3.5-turbo", OPENAI_API_KEY=OPENAI_API_KEY)

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ì•± ê¸°ëŠ¥ ì„¤ëª…ì´ë‚˜ FAQì— ë§ì¶¤)
system_prompt = (
    "You are an assistant for this Streamlit app. Use the following context to answer questions about the app's features, FAQ, or code from the GitHub repo."
    "\n\n{context}"
)
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}"),
    ]
)

# ì²´ì¸ ìƒì„±
question_answer_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(vectorstore.as_retriever(), question_answer_chain)

# ì±„íŒ… íˆìŠ¤í† ë¦¬ ìœ ì§€
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì´ì „ ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥
if user_input := st.chat_input("Ask about the app features or FAQ:"):
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # RAG ì²´ì¸ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        response = rag_chain.invoke({"input": user_input})
        st.markdown(response["answer"])
        st.session_state.messages.append({"role": "assistant", "content": response["answer"]})