import os
import streamlit as st
from openai import OpenAI
from langchain_community.document_loaders import DirectoryLoader, UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
import git
import shutil
import glob
import logging
import time
import menu

st.set_page_config(page_title="ì±—ë´‡ì—ê²Œ ë¬¼ì–´ë³´ê¸°", page_icon="ğŸ¤–", layout="wide")

import os
st.session_state.current_page = os.path.basename(__file__)

menu.menu()

# ë¡œê·¸ì¸ ì²´í¬ ë° ìë™ ë¦¬ë””ë ‰ì…˜
if not st.session_state.get("login_success", False):
    st.warning("âš ï¸ Home í˜ì´ì§€ì—ì„œ ë¨¼ì € ë¡œê·¸ì¸í•´ì£¼ì„¸ìš”.")
    st.error("1ì´ˆ í›„ Home í˜ì´ì§€ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤...")
    time.sleep(1)
    st.switch_page("Home.py")  # Home í˜ì´ì§€ë¡œ ì´ë™
    st.stop()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# =========================
# 0) ìƒìˆ˜ ì„¤ì •
# =========================
REPO_URL = "https://github.com/HUI135/gc-endoscopy-room.git"
BRANCH = "main"

# =========================
# 1) API í‚¤ ì„¤ì • ë° ê²€ì‚¬
# =========================
# Streamlit secretsì—ì„œ API í‚¤ë¥¼ ì•ˆì „í•˜ê²Œ ë¡œë“œí•©ë‹ˆë‹¤.
try:
    OPENAI_API_KEY = st.secrets["gpt"]["openai_api_key"]
    os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
except (KeyError, TypeError):
    st.error("âš ï¸ OpenAI API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Streamlit secrets ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë° ì—°ê²° í…ŒìŠ¤íŠ¸
try:
    client = OpenAI(api_key=OPENAI_API_KEY)
    client.models.list() 
    logging.info("OpenAI API ì—°ê²°ì— ì„±ê³µí–ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    st.error(f"OpenAI API ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
    logging.error(f"OpenAI API ì—°ê²° ì˜¤ë¥˜: {e}")
    st.stop()

# =========================
# 2) ë°ì´í„° ë¡œë“œ (ìºì‹œ)
# =========================
@st.cache_resource(show_spinner="ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
def load_knowledge_base():
    """
    Git ë¦¬í¬ì§€í† ë¦¬ì—ì„œ ë°ì´í„°ë¥¼ í´ë¡ í•˜ê³ , ë¬¸ì„œë¥¼ ë¡œë“œí•˜ì—¬ ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    repo_path = "./temp_repo"
    try:
        # ê¸°ì¡´ì— í´ë¡ ëœ ë¦¬í¬ì§€í† ë¦¬ê°€ ìˆë‹¤ë©´ ì‚­ì œí•©ë‹ˆë‹¤.
        if os.path.exists(repo_path):
            logging.info(f"ê¸°ì¡´ ë¦¬í¬ì§€í† ë¦¬ ê²½ë¡œ '{repo_path}'ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.")
            shutil.rmtree(repo_path)

        # Git ë¦¬í¬ì§€í† ë¦¬ë¥¼ í´ë¡ í•©ë‹ˆë‹¤.
        logging.info(f"'{REPO_URL}' ë¦¬í¬ì§€í† ë¦¬ì˜ '{BRANCH}' ë¸Œëœì¹˜ë¥¼ í´ë¡ í•©ë‹ˆë‹¤.")
        git.Repo.clone_from(REPO_URL, repo_path, branch=BRANCH)

        # í…ìŠ¤íŠ¸ ê¸°ë°˜ íŒŒì¼ë§Œ ë¡œë“œí•˜ë„ë¡ glob íŒ¨í„´ì„ êµ¬ì²´ì ìœ¼ë¡œ ì§€ì •í•©ë‹ˆë‹¤.
        # ì´ë ‡ê²Œ í•˜ë©´ ì´ë¯¸ì§€ë‚˜ ë‹¤ë¥¸ ë°”ì´ë„ˆë¦¬ íŒŒì¼ ë¡œë“œ ì‹œ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ë¥¼ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        loader = DirectoryLoader(
            repo_path,
            glob="**/*.{py,md,txt,html,css,js}", # í…ìŠ¤íŠ¸ ê¸°ë°˜ íŒŒì¼ í™•ì¥ì ì§€ì •
            loader_cls=UnstructuredFileLoader, # ë‹¤ì–‘í•œ íŒŒì¼ ìœ í˜•ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ë¡œë”
            show_progress=True,
            recursive=True,
            use_multithreading=True
        )
        docs = loader.load()

        if not docs:
            st.warning("âš ï¸ í”„ë¡œì íŠ¸ì—ì„œ ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë¦¬í¬ì§€í† ë¦¬ì— í…ìŠ¤íŠ¸ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            logging.warning("DirectoryLoaderê°€ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return None

        # ë¡œë“œëœ ë¬¸ì„œ ì†ŒìŠ¤ ë¡œê¹…
        loaded_sources = [doc.metadata.get('source', 'N/A') for doc in docs]
        logging.info(f"{len(loaded_sources)}ê°œì˜ ë¬¸ì„œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤: {loaded_sources[:5]}") # ì²˜ìŒ 5ê°œë§Œ ë¡œê·¸

        # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)
        logging.info(f"ë¬¸ì„œë¥¼ {len(splits)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")

        # OpenAI ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        vectorstore = FAISS.from_documents(splits, embeddings)
        logging.info("FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„±ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
        
        return vectorstore

    except Exception as e:
        st.error(f"í”„ë¡œì íŠ¸ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        logging.error(f"load_knowledge_base í•¨ìˆ˜ì—ì„œ ì˜ˆì™¸ ë°œìƒ: {e}", exc_info=True)
        return None
    finally:
        # ì„ì‹œ ë¦¬í¬ì§€í† ë¦¬ ì •ë¦¬
        if os.path.exists(repo_path):
            shutil.rmtree(repo_path)
            logging.info("ì„ì‹œ ë¦¬í¬ì§€í† ë¦¬ íŒŒì¼ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")


# =========================
# 3) Streamlit UI ì„¤ì •
# =========================
st.subheader("ğŸ¤– GC ë‚´ì‹œê²½ì‹¤ ì±—ë´‡", divider="rainbow")
st.info("ì´ ì±—ë´‡ì€ GC ë‚´ì‹œê²½ì‹¤ GitHub ë¦¬í¬ì§€í† ë¦¬ì˜ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.\n\nì•±ì˜ ê¸°ëŠ¥, ì½”ë“œ êµ¬ì¡°, í”„ë¡œì íŠ¸ ëª©ì  ë“± ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!")

# ë°ì´í„° ë¡œë“œ ì‹¤í–‰
vectorstore = load_knowledge_base()
if vectorstore is None:
    st.error("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì•±ì„ ë‹¤ì‹œ ì‹œì‘í•˜ê±°ë‚˜ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
    st.stop()

# =========================
# 4) LLM ë° RAG ì²´ì¸ êµ¬ì„±
# =========================
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

system_prompt = (
    "ë‹¹ì‹ ì€ GC ë‚´ì‹œê²½ì‹¤ ì•± í”„ë¡œì íŠ¸ì— ëŒ€í•´ ì„¤ëª…í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
    "ì œê³µëœ í”„ë¡œì íŠ¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. "
    "ê¸°ìˆ ì ì¸ ì§ˆë¬¸ì—ëŠ” ì½”ë“œì˜ ë§¥ë½ì„ íŒŒì•…í•˜ì—¬ ì„¤ëª…í•˜ê³ , ì¼ë°˜ì ì¸ ì§ˆë¬¸ì—ëŠ” ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
    "\n\n"
    "ì°¸ê³  ì •ë³´:\n{context}"
)

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}"),
    ]
)

# LangChain Expression Language (LCEL)ì„ ì‚¬ìš©í•˜ì—¬ ì²´ì¸ êµ¬ì„±
retriever = vectorstore.as_retriever()
question_answer_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(retriever, question_answer_chain)


# =========================
# 5) ì±„íŒ… UI ë¡œì§
# =========================
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì´ì „ ëŒ€í™” ë‚´ìš© í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if user_input := st.chat_input("ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ ë° í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ìƒì„± ë° í‘œì‹œ
    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ì„ ìƒê°í•˜ê³  ìˆì–´ìš”..."):
            try:
                response = rag_chain.invoke({"input": user_input})
                answer = response.get("answer", "ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                answer = "ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë™ì•ˆ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                logging.error(f"RAG ì²´ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            
            st.markdown(answer)
            st.session_state.messages.append({"role": "assistant", "content": answer})

# =========================
# 6) ì»¤ìŠ¤í…€ ìŠ¤íƒ€ì¼ë§
# =========================
st.markdown("""
<style>
    .stChatMessage {
        border-radius: 10px;
        border: 1px solid #e0e0e0;
        padding: 1rem;
        margin-bottom: 1rem;
        box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }
    /* Streamlit 1.36+ ì—ì„œëŠ” data-testid ì†ì„±ì„ ì‚¬ìš©í•©ë‹ˆë‹¤ */
    [data-testid="chat-message-container-user"] {
        background-color: #e7f3ff;
    }
    [data-testid="chat-message-container-assistant"] {
        background-color: #f8f9fa;
    }
</style>
""", unsafe_allow_html=True)
