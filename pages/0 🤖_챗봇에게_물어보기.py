import os
import streamlit as st
from openai import OpenAI
# [ìˆ˜ì •] DirectoryLoader ëŒ€ì‹  Document ê°ì²´ë¥¼ ì§ì ‘ ì‚¬ìš©í•©ë‹ˆë‹¤.
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
import time
import menu
import git
import shutil
import traceback

st.set_page_config(page_title="ì±—ë´‡ì—ê²Œ ë¬¼ì–´ë³´ê¸°", page_icon="ğŸ¤–", layout="wide")

st.session_state.current_page = os.path.basename(__file__)
menu.menu()

# ë¡œê·¸ì¸ ì²´í¬ ë° ìë™ ë¦¬ë””ë ‰ì…˜
if not st.session_state.get("login_success", False):
    st.warning("âš ï¸ Home í˜ì´ì§€ì—ì„œ ë¨¼ì € ë¡œê·¸ì¸í•´ì£¼ì„¸ìš”.")
    st.error("1ì´ˆ í›„ Home í˜ì´ì§€ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤...")
    time.sleep(1)
    st.switch_page("Home.py")
    st.stop()

# =========================
# 0) ìƒìˆ˜ ì„¤ì •
# =========================
REPO_URL = "https://github.com/HUI135/gc-endoscopy-room.git"
BRANCH = "main"

# =========================
# 1) API í‚¤ ì„¤ì • ë° ê²€ì‚¬
# =========================
try:
    OPENAI_API_KEY = st.secrets["gpt"]["openai_api_key"]
    os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
except (KeyError, TypeError):
    st.error("âš ï¸ ì‹œìŠ¤í…œ ì„¤ì • ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
    st.stop()

# OpenAI ì—°ê²° í…ŒìŠ¤íŠ¸
try:
    client = OpenAI(api_key=OPENAI_API_KEY)
    client.models.list()
except Exception as e:
    st.error(f"ì‹œìŠ¤í…œ ì—°ê²° ì˜¤ë¥˜: {e}")
    st.stop()

# =========================
# 2) ë°ì´í„° ë¡œë“œ (ìºì‹œ)
# =========================
@st.cache_resource(show_spinner="ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ëŠ” ì¤‘...")
def load_knowledge_base():
    repo_path = "./temp_repo"
    try:
        if os.path.exists(repo_path):
            shutil.rmtree(repo_path)

        git.Repo.clone_from(REPO_URL, repo_path, branch=BRANCH)

        # [í•µì‹¬ ìˆ˜ì •] DirectoryLoader ëŒ€ì‹  íŒŒì¼ì„ ì§ì ‘ ì½ì–´ Document ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        docs = []
        file_extensions_to_load = ['.py', '.md', '.txt']
        
        loaded_files_list = []

        for root, _, files in os.walk(repo_path):
            if ".git" in root:
                continue
            for file_name in files:
                if any(file_name.endswith(ext) for ext in file_extensions_to_load):
                    file_path = os.path.join(root, file_name)
                    try:
                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                            content = f.read()
                        doc = Document(page_content=content, metadata={"source": file_path})
                        docs.append(doc)
                        loaded_files_list.append(file_path)
                    except Exception as e:
                        st.warning(f"'{file_path}' íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        if not docs:
            st.warning("âš ï¸ ë¦¬í¬ì§€í† ë¦¬ì—ì„œ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return None

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        if not splits:
            st.warning("âš ï¸ íŒŒì¼ì„ ë¶„ì„ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return None

        embeddings = OpenAIEmbeddings(model="text-embedding-3-small", api_key=OPENAI_API_KEY)
        vectorstore = FAISS.from_documents(splits, embeddings)
        return vectorstore
    except Exception as e:
        st.error(f"âŒ ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        st.code(traceback.format_exc())
        return None
    finally:
        # ì‘ì—… ì™„ë£Œ í›„ ì„ì‹œ í´ë” ì •ë¦¬
        if os.path.exists(repo_path):
            shutil.rmtree(repo_path)

# =========================
# 3) Streamlit UI ì„¤ì •
# =========================
st.header("ğŸ¤– ì±—ë´‡ì—ê²Œ ë¬¼ì–´ë³´ê¸°", divider='rainbow')
st.info("ì±—ë´‡ì—ê²Œ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”! ì˜ˆ: ì•± ê¸°ëŠ¥, í”„ë¡œì íŠ¸ ì •ë³´ ë“±")
st.write()
st.divider()

vectorstore = load_knowledge_base()

if vectorstore is None:
    st.error("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìœ„ì˜ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì—¬ ì›ì¸ì„ íŒŒì•…í•˜ê±°ë‚˜ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
    st.stop()

# =========================
# 4) ì±—ë´‡ ì„¤ì •
# =========================
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, api_key=OPENAI_API_KEY)
system_prompt = (
    "You are a friendly assistant for the GC Endoscopy app. "
    "Answer questions clearly and simply using the provided project information.\n\n{context}"
)
prompt = ChatPromptTemplate.from_messages(
    [("system", system_prompt), ("human", "{input}")]
)
question_answer_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(vectorstore.as_retriever(), question_answer_chain)

# =========================
# 5) ì±„íŒ… UI
# =========================
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ê°•ë‚¨ì„¼í„° ë‚´ì‹œê²½ì‹¤ ì‹œìŠ¤í…œì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ë¬¼ì–´ë³´ì„¸ìš”! ğŸ˜Š"}
    ]

# ì´ì „ ëŒ€í™” ë‚´ìš© í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"], avatar="ğŸ¥" if message["role"] == "assistant" else None):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if user_input := st.chat_input("ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì´ ì•±ì€ ë¬´ì—‡ì¸ê°€ìš”?)"):
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    with st.chat_message("assistant", avatar="ğŸ¥"):
        with st.spinner("ë‹µë³€ì„ ì¤€ë¹„í•˜ëŠ” ì¤‘..."):
            try:
                response = rag_chain.invoke({"input": user_input})
                answer = response["answer"]
            except Exception as e:
                answer = f"ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
            st.markdown(answer)
            st.session_state.messages.append({"role": "assistant", "content": answer})

# ìŠ¤íƒ€ì¼ë§
st.markdown(
    """
    <style>
    .stChatMessage {
        border-radius: 12px;
        padding: 12px;
        margin-bottom: 12px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    [data-testid="chat-message-container-user"] {
        background-color: #d9e6ff;
    }
    [data-testid="chat-message-container-assistant"] {
        background-color: #f5f5f5;
    }
    .stTitle {
        color: #2c3e50;
        font-weight: bold;
    }
    </style>
    """,
    unsafe_allow_html=True
)
