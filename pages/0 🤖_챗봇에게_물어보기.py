import os
import streamlit as st
from openai import OpenAI
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
import time
import menu
import git
import shutil
import traceback

st.set_page_config(page_title="ì±—ë´‡ì—ê²Œ ë¬¼ì–´ë³´ê¸°", page_icon="ğŸ¤–", layout="wide")

st.session_state.current_page = os.path.basename(__file__)
menu.menu()

# ë¡œê·¸ì¸ ì²´í¬ ë° ìë™ ë¦¬ë””ë ‰ì…˜
if not st.session_state.get("login_success", False):
    st.warning("âš ï¸ Home í˜ì´ì§€ì—ì„œ ë¨¼ì € ë¡œê·¸ì¸í•´ì£¼ì„¸ìš”.")
    st.error("1ì´ˆ í›„ Home í˜ì´ì§€ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤...")
    time.sleep(1)
    st.switch_page("Home.py")
    st.stop()

# =========================
# 0) ìƒìˆ˜ ì„¤ì •
# =========================
REPO_URL = "https://github.com/HUI135/gc-endoscopy-room.git"
BRANCH = "main"

# =========================
# 1) API í‚¤ ì„¤ì • ë° ê²€ì‚¬
# =========================
try:
    OPENAI_API_KEY = st.secrets["gpt"]["openai_api_key"]
    os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
except (KeyError, TypeError):
    st.error("âš ï¸ ì‹œìŠ¤í…œ ì„¤ì • ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
    st.stop()

# OpenAI ì—°ê²° í…ŒìŠ¤íŠ¸
try:
    client = OpenAI(api_key=OPENAI_API_KEY)
    client.models.list()
except Exception as e:
    st.error(f"ì‹œìŠ¤í…œ ì—°ê²° ì˜¤ë¥˜: {e}")
    st.stop()

# =========================
# 2) ë°ì´í„° ë¡œë“œ (ìºì‹œ)
# =========================
@st.cache_resource(show_spinner="ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ëŠ” ì¤‘...")
def load_knowledge_base():
    repo_path = "./temp_repo"
    try:
        if os.path.exists(repo_path):
            shutil.rmtree(repo_path, ignore_errors=True)

        git.Repo.clone_from(REPO_URL, repo_path, branch=BRANCH)

        docs = []
        file_extensions_to_load = ['.py', '.md', '.txt']
        
        for root, _, files in os.walk(repo_path):
            if ".git" in root:
                continue
            for file_name in files:
                if any(file_name.endswith(ext) for ext in file_extensions_to_load):
                    file_path = os.path.join(root, file_name)
                    try:
                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                            content = f.read()
                        doc = Document(page_content=content, metadata={"source": file_path})
                        docs.append(doc)
                    except Exception as e:
                        st.warning(f"'{file_name}' íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        if not docs:
            st.warning("âš ï¸ ë¦¬í¬ì§€í† ë¦¬ì—ì„œ .py, .md, .txt íŒŒì¼ì„ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë¦¬í¬ì§€í† ë¦¬ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.")
            return None

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        if not splits:
            st.warning("âš ï¸ íŒŒì¼ì„ ë¶„ì„ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return None

        embeddings = OpenAIEmbeddings(model="text-embedding-3-small", api_key=OPENAI_API_KEY)
        vectorstore = FAISS.from_documents(splits, embeddings)
        return vectorstore
    except Exception as e:
        st.error(f"âŒ ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        st.code(traceback.format_exc())
        return None
    finally:
        if os.path.exists(repo_path):
            shutil.rmtree(repo_path, ignore_errors=True)

# =========================
# 3) Streamlit UI ì„¤ì •
# =========================
st.header("ğŸ¤– ì±—ë´‡ì—ê²Œ ë¬¼ì–´ë³´ê¸°", divider='rainbow')
st.info("ì±—ë´‡ì—ê²Œ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”! ì˜ˆ: ì•± ê¸°ëŠ¥, í”„ë¡œì íŠ¸ ì •ë³´ ë“±")
st.write()
st.divider()

vectorstore = load_knowledge_base()

if vectorstore is None:
    st.error("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìœ„ì˜ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì—¬ ì›ì¸ì„ íŒŒì•…í•˜ê±°ë‚˜ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
    st.stop()

# =========================
# 4) ì±—ë´‡ ì„¤ì •
# =========================
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, api_key=OPENAI_API_KEY)
system_prompt = (
    "You are a friendly assistant for the GC Endoscopy app, designed to help users of the Gangnam Center endoscopy services. "
    "Answer questions clearly and simply, focusing on how to use the app (e.g., booking appointments, viewing hospital information, submitting requests like schedule or room assignment changes) "
    "or general information about endoscopy procedures. "
    "Only if the user explicitly states 'I am an admin' or 'administrator' (e.g., 'I am an admin, how do I manage schedules?'), "
    "provide clear and simple answers about admin features (e.g., managing schedules, assigning rooms) based on relevant project information. "
    "Otherwise, do not mention admin-specific features, as they are password-protected and not accessible to general users. "
    "Use the provided project information only when relevant to the user's question.\n\n{context}"
)
prompt = ChatPromptTemplate.from_messages(
    [("system", system_prompt), ("human", "{input}")]
)
question_answer_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(vectorstore.as_retriever(), question_answer_chain)

# =========================
# 5) ì±„íŒ… UI
# =========================
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ê°•ë‚¨ì„¼í„° ë‚´ì‹œê²½ì‹¤ ì‹œìŠ¤í…œì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ë¬¼ì–´ë³´ì„¸ìš”! ğŸ˜Š"}
    ]

# ì±„íŒ… ì˜ì—­ì„ ë°•ìŠ¤ë¡œ ê°ì‹¸ê¸°
chat_container = st.container()
with chat_container:
    # ì´ì „ ëŒ€í™” ë‚´ìš© í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"], avatar="ğŸ¥" if message["role"] == "assistant" else None):
            st.markdown(message["content"])

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if user_input := st.chat_input("ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì´ ì•±ì€ ë¬´ì—‡ì¸ê°€ìš”?)"):
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        with st.chat_message("assistant", avatar="ğŸ¥"):
            with st.spinner("ë‹µë³€ì„ ì¤€ë¹„í•˜ëŠ” ì¤‘..."):
                try:
                    response = rag_chain.invoke({"input": user_input})
                    answer = response["answer"]
                except Exception as e:
                    answer = f"ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                st.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})

# ìŠ¤íƒ€ì¼ë§
st.markdown(
    """
    <style>
    /* ëŒ€í™”ì°½ ë°•ìŠ¤ */
    .stChatMessage {
        border-radius: 12px;
        padding: 12px;
        margin-bottom: 12px;
        border: 1px solid #e0e0e0; /* ì–‡ì€ íšŒìƒ‰ í…Œë‘ë¦¬ */
        box-shadow: 0 2px 4px rgba(0,0,0,0.1); /* ë¶€ë“œëŸ¬ìš´ ê·¸ë¦¼ì */
    }
    [data-testid="chat-message-container-user"] {
        background-color: #d9e6ff; /* ì—°í•œ íŒŒë‘ */
    }
    [data-testid="chat-message-container-assistant"] {
        background-color: #f5f5f5; /* ì—°í•œ íšŒìƒ‰ */
    }
    /* ì…ë ¥ì°½ ë°•ìŠ¤ */
    [data-testid="stTextInput"] {
        background-color: #ffffff; /* í°ìƒ‰ ë°°ê²½ */
        border: 1px solid #e0e0e0; /* ì–‡ì€ íšŒìƒ‰ í…Œë‘ë¦¬ */
        border-radius: 8px; /* ë‘¥ê·¼ í…Œë‘ë¦¬ */
        box-shadow: 0 4px 4px rgba(0,0,0,0.1); /* ë¶€ë“œëŸ¬ìš´ ê·¸ë¦¼ì */
        padding: 10px;
    }
    </style>
    """,
    unsafe_allow_html=True
)