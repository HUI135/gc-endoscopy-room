import os
import streamlit as st
from openai import OpenAI
from langchain_community.document_loaders import GitLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.vectorstores import FAISS
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
import shutil

# =========================
# 0) ìƒìˆ˜ ì„¤ì •
# =========================
REPO_URL = "https://github.com/HUI135/gc-endoscopy-room.git"
BRANCH = "main"

# =========================
# 1) API í‚¤ ì„¤ì • ë° ê²€ì‚¬
# =========================
if "gpt" not in st.secrets or "openai_api_key" not in st.secrets["gpt"]:
    st.error("âš ï¸ ì‹œìŠ¤í…œ ì„¤ì • ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
    st.stop()

OPENAI_API_KEY = st.secrets["gpt"]["openai_api_key"].strip()
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

# OpenAI ì—°ê²° í…ŒìŠ¤íŠ¸
try:
    client = OpenAI(api_key=OPENAI_API_KEY)
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": "hello"}],
    )
    st.write("OpenAI ì—°ê²° ì„±ê³µ:", resp.choices[0].message.content)
except Exception as e:
    st.error(f"OpenAI ì—°ê²° ì˜¤ë¥˜: {e}")
    st.stop()

# =========================
# 2) ë°ì´í„° ë¡œë“œ (ìºì‹œ)
# =========================
@st.cache_resource(show_spinner="ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ëŠ” ì¤‘...")
def load_knowledge_base():
    repo_path = "./temp_repo"
    try:
        # ê¸°ì¡´ í´ë¡  ì œê±°
        if os.path.exists(repo_path):
            shutil.rmtree(repo_path, ignore_errors=True)

        # GitLoaderë¡œ í´ë¡  ë° íŒŒì¼ ë¡œë“œ
        loader = GitLoader(
            clone_url=REPO_URL,
            repo_path=repo_path,
            branch=BRANCH,
            file_filter=lambda p: (
                p.endswith((".py", ".md", ".txt")) and
                ".git" not in p and
                ".gitignore" not in p and
                "submodule" not in p.lower()
            )
        )
        docs = loader.load()

        if not docs:
            st.error("âš ï¸ í”„ë¡œì íŠ¸ì—ì„œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. Streamlit Cloudì˜ 'Manage app'ì—ì„œ ë¡œê·¸ë¥¼ í™•ì¸í•˜ê±°ë‚˜ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
            return None

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        embeddings = OpenAIEmbeddings(model="text-embedding-3-small", api_key=OPENAI_API_KEY)
        vectorstore = FAISS.from_documents(splits, embeddings)
        return vectorstore
    except Exception as e:
        st.error("âš ï¸ í”„ë¡œì íŠ¸ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. Streamlit Cloudì˜ 'Manage app'ì—ì„œ ë¡œê·¸ë¥¼ í™•ì¸í•˜ê±°ë‚˜ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
        return None

# =========================
# 3) Streamlit UI
# =========================
st.subheader("ğŸ¤– ì±—ë´‡ì—ê²Œ ë¬¼ì–´ë³´ê¸°", divider="rainbow")

# ë°ì´í„° ë¡œë“œ
vectorstore = load_knowledge_base()
if vectorstore is None:
    st.stop()

# =========================
# 4) LLM & ì²´ì¸ êµ¬ì„±
# =========================
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0,
    api_key=OPENAI_API_KEY,
)

system_prompt = (
    "You are a friendly assistant for the GC Endoscopy app. "
    "Answer questions clearly and simply using the provided project information.\n\n{context}"
)
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}"),
    ]
)

question_answer_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(vectorstore.as_retriever(), question_answer_chain)

# =========================
# 5) ì±„íŒ… UI
# =========================
if "messages" not in st.session_state:
    st.session_state.messages = []

for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

# ì…ë ¥ì°½
if user_input := st.chat_input("ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì•± ê¸°ëŠ¥, í”„ë¡œì íŠ¸ ì •ë³´ ë“±)"):
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ì„ ì¤€ë¹„í•˜ëŠ” ì¤‘..."):
            try:
                response = rag_chain.invoke({"input": user_input})
                answer = response["answer"]
            except Exception as e:
                answer = f"ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
            st.markdown(answer)
            st.session_state.messages.append({"role": "assistant", "content": answer})

# ìŠ¤íƒ€ì¼ë§
st.markdown(
    """
    <style>
    .stChatMessage {
        border-radius: 12px;
        padding: 12px;
        margin-bottom: 12px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    .stChatMessage[data-user="user"] {
        background-color: #d9e6ff;
        border-left: 4px solid #1e90ff;
    }
    .stChatMessage[data-user="assistant"] {
        background-color: #f5f5f5;
        border-left: 4px solid #2ecc71;
    }
    .stTitle {
        color: #2c3e50;
        font-weight: bold;
    }
    </style>
    """,
    unsafe_allow_html=True
)