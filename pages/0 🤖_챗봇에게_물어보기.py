import os
import streamlit as st
from openai import OpenAI
from langchain_community.document_loaders import GitLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.vectorstores import FAISS
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
import menu

st.set_page_config(page_title="ì±—ë´‡ì—ê²Œ ë¬¼ì–´ë³´ê¸°", page_icon="ğŸ¤–", layout="wide")

import os
st.session_state.current_page = os.path.basename(__file__)

menu.menu()

# ë¡œê·¸ì¸ ì²´í¬ ë° ìë™ ë¦¬ë””ë ‰ì…˜
if not st.session_state.get("login_success", False):
    st.warning("âš ï¸ Home í˜ì´ì§€ì—ì„œ ë¨¼ì € ë¡œê·¸ì¸í•´ì£¼ì„¸ìš”.")
    st.error("1ì´ˆ í›„ Home í˜ì´ì§€ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤...")
    time.sleep(1)
    st.switch_page("Home.py")  # Home í˜ì´ì§€ë¡œ ì´ë™
    st.stop()

# =========================
# 0) ìƒìˆ˜ ì„¤ì •
# =========================
REPO_URL = "https://github.com/HUI135/gc-endoscopy-room.git"
BRANCH = "main"

# =========================
# 1) API í‚¤ ì„¤ì • ë° ê²€ì‚¬
# =========================
if "gpt" not in st.secrets or "openai_api_key" not in st.secrets["gpt"]:
    st.error("âš ï¸ ì‹œìŠ¤í…œ ì„¤ì • ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
    st.stop()

OPENAI_API_KEY = st.secrets["gpt"]["openai_api_key"].strip()
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

# OpenAI ì—°ê²° í…ŒìŠ¤íŠ¸
try:
    client = OpenAI(api_key=OPENAI_API_KEY)
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": "hello"}],
    )
except Exception as e:
    st.error(f"ì‹œìŠ¤í…œ ì—°ê²° ì˜¤ë¥˜: {e}")
    st.stop()

# =========================
# 2) ë°ì´í„° ë¡œë“œ (ìºì‹œ)
# =========================
@st.cache_resource(show_spinner="ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ëŠ” ì¤‘...")
def load_knowledge_base():
    repo_path = "./temp_repo"
    loader = GitLoader(
        clone_url=REPO_URL,
        repo_path=repo_path,
        branch=BRANCH,
        file_filter=lambda p: p.endswith((".py", ".md", ".txt"))
    )
    docs = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small", api_key=OPENAI_API_KEY)
    vectorstore = FAISS.from_documents(splits, embeddings)
    return vectorstore

# =========================
# 3) Streamlit UI ì„¤ì •
# =========================

# ë©”ì¸ ë ˆì´ì•„ì›ƒ
st.header("ğŸ¤– ì±—ë´‡ì—ê²Œ ë¬¼ì–´ë³´ê¸°", divider='rainbow')
st.write("- ì±—ë´‡ì—ê²Œ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”! ì˜ˆ: ì•± ê¸°ëŠ¥, í”„ë¡œì íŠ¸ ì •ë³´ ë“±")
st.write()

# ë°ì´í„° ë¡œë“œ
with st.spinner("ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ëŠ” ì¤‘..."):
    vectorstore = load_knowledge_base()

# =========================
# 4) ì±—ë´‡ ì„¤ì •
# =========================
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, api_key=OPENAI_API_KEY)

system_prompt = (
    "You are a friendly assistant for the GC Endoscopy app. "
    "Answer questions clearly and simply using the provided project information.\n\n{context}"
)
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}"),
    ]
)

question_answer_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(vectorstore.as_retriever(), question_answer_chain)

# =========================
# 5) ì±„íŒ… UI
# =========================
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! GC Endoscopy í”„ë¡œì íŠ¸ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ë¬¼ì–´ë³´ì„¸ìš”! ğŸ˜Š"}
    ]

# ì±„íŒ… ì˜ì—­
chat_container = st.container()
with chat_container:
    for m in st.session_state.messages:
        with st.chat_message(m["role"], avatar="ğŸ¥" if m["role"] == "assistant" else None):
            st.markdown(m["content"])

# ì…ë ¥ì°½
user_input = st.chat_input("ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì´ ì•±ì€ ë¬´ì—‡ì¸ê°€ìš”?)")
if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    with chat_container:
        with st.chat_message("user"):
            st.markdown(user_input)

        with st.chat_message("assistant", avatar="ğŸ¥"):
            with st.spinner("ë‹µë³€ì„ ì¤€ë¹„í•˜ëŠ” ì¤‘..."):
                try:
                    response = rag_chain.invoke({"input": user_input})
                    answer = response["answer"]
                except Exception as e:
                    answer = f"ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                st.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})

# ìŠ¤íƒ€ì¼ë§
st.markdown(
    """
    <style>
    .stChatMessage {
        border-radius: 12px;
        padding: 12px;
        margin-bottom: 12px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    .stChatMessage[data-user="user"] {
        background-color: #d9e6ff;
        border-left: 4px solid #1e90ff;
    }
    .stChatMessage[data-user="assistant"] {
        background-color: #f5f5f5;
        border-left: 4px solid #2ecc71;
    }
    .stTitle {
        color: #2c3e50;
        font-weight: bold;
    }
    </style>
    """,
    unsafe_allow_html=True
)