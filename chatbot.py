import os
import streamlit as st
from openai import OpenAI
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
import git
import shutil
import traceback

# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼)
@st.cache_resource(show_spinner="ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ëŠ” ì¤‘...")
def load_knowledge_base():
    repo_path = "./temp_repo"
    try:
        if os.path.exists(repo_path):
            shutil.rmtree(repo_path, ignore_errors=True)

        git.Repo.clone_from("https://github.com/HUI135/gc-endoscopy-room.git", repo_path, branch="main")

        docs = []
        file_extensions_to_load = ['.py', '.md', '.txt']
        
        for root, _, files in os.walk(repo_path):
            if ".git" in root:
                continue
            for file_name in files:
                if any(file_name.endswith(ext) for ext in file_extensions_to_load):
                    file_path = os.path.join(root, file_name)
                    try:
                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                            content = f.read()
                        doc = Document(page_content=content, metadata={"source": file_path})
                        docs.append(doc)
                    except Exception as e:
                        st.warning(f"'{file_name}' íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        if not docs:
            st.warning("âš ï¸ ë¦¬í¬ì§€í† ë¦¬ì—ì„œ .py, .md, .txt íŒŒì¼ì„ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë¦¬í¬ì§€í† ë¦¬ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.")
            return None

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        if not splits:
            st.warning("âš ï¸ íŒŒì¼ì„ ë¶„ì„ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return None

        embeddings = OpenAIEmbeddings(model="text-embedding-3-small", api_key=st.secrets["gpt"]["openai_api_key"])
        vectorstore = FAISS.from_documents(splits, embeddings)
        return vectorstore
    except Exception as e:
        st.error(f"âŒ ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        st.code(traceback.format_exc())
        return None
    finally:
        if os.path.exists(repo_path):
            shutil.rmtree(repo_path, ignore_errors=True)

# ì±—ë´‡ ì„¤ì • ë° ë Œë”ë§ í•¨ìˆ˜
def render_chatbot():
    # API í‚¤ ì„¤ì • ë° ê²€ì‚¬
    try:
        OPENAI_API_KEY = st.secrets["gpt"]["openai_api_key"]
        os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
    except (KeyError, TypeError):
        st.error("âš ï¸ ì‹œìŠ¤í…œ ì„¤ì • ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
        return

    # OpenAI ì—°ê²° í…ŒìŠ¤íŠ¸
    try:
        client = OpenAI(api_key=OPENAI_API_KEY)
        client.models.list()
    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì—°ê²° ì˜¤ë¥˜: {e}")
        return

    # ë°ì´í„° ë¡œë“œ
    vectorstore = load_knowledge_base()
    if vectorstore is None:
        st.error("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìœ„ì˜ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì—¬ ì›ì¸ì„ íŒŒì•…í•˜ê±°ë‚˜ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
        return

    # ì±—ë´‡ ì„¤ì •
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, api_key=OPENAI_API_KEY)
    system_prompt = (
        "You are a friendly assistant for the GC Endoscopy app, designed to help general users of the Gangnam Center endoscopy services. "
        "Answer questions clearly and simply, focusing solely on how to use the app for general users (e.g., booking appointments, viewing hospital information, submitting requests like schedule or room assignment changes) "
        "or general information about endoscopy procedures. "
        "Do not mention or provide information about admin-specific features (e.g., schedule management, room assignment, or any direct system modifications) unless the user explicitly states 'I am an admin' or 'administrator' (e.g., 'I am an admin, how do I manage schedules?') and is in admin mode (st.session_state.admin_mode=True). "
        "Admin-specific features are password-protected and not accessible to general users, so they must not be referenced in responses to general users, even if keywords like 'schedule' or 'room assignment' are mentioned. "
        "For general users, focus on request submission features (e.g., submitting a schedule change request or room assignment request). "
        "Use the provided project information only when relevant to the user's question, and exclude admin-related files (e.g., '4 ìŠ¤ì¼€ì¤„_ê´€ë¦¬.py', '5 ìŠ¤ì¼€ì¤„_ë°°ì •.py', '6 ë°©_ë°°ì •.py', '7 ë°©_ë°°ì •_ë³€ê²½.py') unless explicitly requested by an admin in admin mode.\n\n{context}"
    )
    prompt = ChatPromptTemplate.from_messages(
        [("system", system_prompt), ("human", "{input}")]
    )
    question_answer_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(vectorstore.as_retriever(), question_answer_chain)

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ê°•ë‚¨ì„¼í„° ë‚´ì‹œê²½ì‹¤ ì‹œìŠ¤í…œì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ë¬¼ì–´ë³´ì„¸ìš”! ğŸ˜Š"}
        ]
    
    # <<-- ë³€ê²½ì  1: ì±—ë´‡ì„ ì‚¬ì´ë“œë°”ì— ë°°ì¹˜
    with st.sidebar:
        with st.popover("ğŸ¤– ì±—ë´‡ ì—´ê¸°"):
            # íŒì—… ë‚´ë¶€ì— ëŒ€í™” ê¸°ë¡ í‘œì‹œ
            for message in st.session_state.messages:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])

            # <<-- ë³€ê²½ì  2: ì‚¬ìš©ì ì…ë ¥ì„ ë°›ê¸° ìœ„í•´ st.chat_input ì‚¬ìš©
            if user_input := st.chat_input("ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”..."):
                # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€í•˜ê³  í™”ë©´ì— í‘œì‹œ
                st.session_state.messages.append({"role": "user", "content": user_input})
                with st.chat_message("user"):
                    st.markdown(user_input)
                
                # ì±—ë´‡ ì‘ë‹µ ìƒì„± ë° í‘œì‹œ
                with st.chat_message("assistant"):
                    with st.spinner("ë‹µë³€ì„ ìƒê°í•˜ê³  ìˆì–´ìš”..."):
                        # ê´€ë¦¬ì ëª¨ë“œ í™•ì¸ ë¡œì§ (ê¸°ì¡´ê³¼ ìœ ì‚¬)
                        is_admin_query = "i am an admin" in user_input.lower() or "administrator" in user_input.lower()
                        is_admin_mode = st.session_state.get("admin_mode", False)

                        if is_admin_query and not is_admin_mode:
                            answer = "ê´€ë¦¬ì ê¸°ëŠ¥ì— ì ‘ê·¼í•˜ë ¤ë©´ ë¨¼ì € ê´€ë¦¬ì ëª¨ë“œë¡œ ì „í™˜í•´ì£¼ì„¸ìš”."
                        else:
                            try:
                                response = rag_chain.invoke({"input": user_input})
                                answer = response["answer"]
                            except Exception as e:
                                answer = f"ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                        
                        st.markdown(answer)

                # ì±—ë´‡ ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
                st.session_state.messages.append({"role": "assistant", "content": answer})
                
                # <<-- ë³€ê²½ì  3: popover ë‚´ë¶€ì—ì„œ ìƒí˜¸ì‘ìš© í›„ st.rerun()ì„ í˜¸ì¶œí•´ UIë¥¼ ì¦‰ì‹œ ì—…ë°ì´íŠ¸
                st.rerun()

# ë©”ì¸ ì•± ë¡œì§
def main():
    st.title("ê°•ë‚¨ì„¼í„° ë‚´ì‹œê²½ì‹¤ ì‹œìŠ¤í…œ")
    st.write("ì‚¬ì´ë“œë°”ì˜ '>')ë¥¼ ëˆ„ë¥´ê³ , ì±—ë´‡ ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ìƒë‹´ì„ ì‹œì‘í•˜ì„¸ìš”.")
    render_chatbot()

if __name__ == "__main__":
    main()